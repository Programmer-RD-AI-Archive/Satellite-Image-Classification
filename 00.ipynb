{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf0b148c-d9be-4465-855b-1d58ee7ba0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import os,cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import *\n",
    "import torch,torchvision\n",
    "from tqdm import tqdm\n",
    "device = 'cuda'\n",
    "PROJECT_NAME = 'Satellite-Image-Classification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59b088cb-720a-44c7-89c8-5d7b2ea93f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    idx = -1\n",
    "    labels = {}\n",
    "    data = []\n",
    "    for folder in tqdm(os.listdir('./data/')):\n",
    "        idx += 1\n",
    "        labels[folder] = idx\n",
    "        for file in os.listdir(f'./data/{folder}'):\n",
    "            file = f'./data/{folder}/{file}'\n",
    "            img = cv2.imread(file)\n",
    "            img = cv2.resize(img,(112,112))\n",
    "            data.append([img/255.0,labels[folder]])\n",
    "    np.random.shuffle(data)\n",
    "    X = []\n",
    "    y = []\n",
    "    for d in data:\n",
    "        X.append(d[0])\n",
    "        y.append(d[1])\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.0625)\n",
    "    return data,X,y,idx,labels,X_train,X_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3e48153-a210-44b7-8fbf-cc6aacd2046d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:02<00:00,  1.66it/s]\n"
     ]
    }
   ],
   "source": [
    "data,X,y,idx,labels,X_train,X_test,y_train,y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56426654-86bf-4d69-aa82-81ddddb4f421",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(np.array(X_train)).view(-1,3,112,112).to(device).float()\n",
    "X_test = torch.from_numpy(np.array(X_test)).view(-1,3,112,112).to(device).float()\n",
    "y_train = torch.from_numpy(np.array(y_train)).to(device)\n",
    "y_test = torch.from_numpy(np.array(y_test)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de1d7ae0-3d49-4338-89e9-4fd98a0723fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_r = {}\n",
    "for l_key,l_val in zip(labels.keys(),labels.values()):\n",
    "    labels_r[l_val] = l_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "851b508f-8106-4882-833c-d1ea8df94386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'green_area': 0, 'cloudy': 1, 'desert': 2, 'water': 3}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67ecc48a-f951-40a0-8127-19dc51ee81c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5279"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71009b0b-b323-473f-8c9d-4f61e7a8d8ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5279"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffdd8720-792c-4dea-95b4-dd7829cbd03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12,6))\n",
    "# plt.title(f'{labels_r[int(y_test[2])]}')\n",
    "# plt.imshow(X_test[2].view(112,112,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25521d53-980a-46e7-a0f8-ac8eb4c1ccac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(model,X,y,criterion):\n",
    "    preds = model(X)\n",
    "    loss = criterion(preds,y)\n",
    "    return loss.item()\n",
    "def get_accuracy(model,X,y):\n",
    "    total = -1\n",
    "    correct = -1\n",
    "    preds = model(X)\n",
    "    for y_batch,pred in zip(y,preds):\n",
    "        pred = torch.argmax(pred)\n",
    "        if y_batch == pred:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    acc = round(correct/total,3)*100\n",
    "    return acc\n",
    "def predict(model):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    for img in os.listdir('./test_data/'):\n",
    "        img_name = img\n",
    "        img = cv2.imread(f'./test_data/{img}')\n",
    "        img = cv2.resize(img,(112,112))\n",
    "        img = img / 255.0\n",
    "        pred = model(torch.from_numpy(np.array(img)).view(1,3,112,112).float().to(device))\n",
    "        pred = torch.argmax(pred)\n",
    "        plt.figure(figsize=(12,7))\n",
    "        plt.imshow(img)\n",
    "        plt.title(f'{labels_r[int(pred)]}')\n",
    "        plt.savefig(f'./preds/{img_name}')\n",
    "        plt.close()\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12815db0-846a-4aee-ad99-76dc6a91f2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.activation = ReLU()\n",
    "        self.max_pool2d = MaxPool2d((2,2),(2,2))\n",
    "        self.conv1 = Conv2d(3,6,(3,3))\n",
    "        self.conv1batchnorm = BatchNorm2d(6)\n",
    "        self.conv2 = Conv2d(6,12,(3,3))\n",
    "        self.conv2batchnorm = BatchNorm2d(12)\n",
    "        self.conv3 = Conv2d(12,24,(3,3))\n",
    "        self.conv3batchnorm = BatchNorm2d(24)\n",
    "        self.conv4 = Conv2d(24,48,(3,3))\n",
    "        self.conv4batchnorm = BatchNorm2d(48)\n",
    "        self.conv5 = Conv2d(48,96,(3,3))\n",
    "        self.conv5batchnorm = BatchNorm2d(96)\n",
    "        self.linear1 = Linear(96*1*1,128)\n",
    "        self.linear1batchnorm = BatchNorm1d(128)\n",
    "        self.linear2 = Linear(128,256)\n",
    "        self.linear2batchnorm = BatchNorm1d(256)\n",
    "        self.linear3 = Linear(256,512)\n",
    "        self.linear3batchnorm = BatchNorm1d(512)\n",
    "        self.linear4 = Linear(512,1024)\n",
    "        self.linear4batchnorm = BatchNorm1d(1024)\n",
    "        self.linear5 = Linear(1024,512)\n",
    "        self.linear5batchnorm = BatchNorm1d(512)\n",
    "        self.output = Linear(512,4)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        preds = X\n",
    "        preds = self.activation(self.max_pool2d(self.conv1batchnorm(self.conv1(preds))))\n",
    "        preds = self.activation(self.max_pool2d(self.conv2batchnorm(self.conv2(preds))))\n",
    "        preds = self.activation(self.max_pool2d(self.conv3batchnorm(self.conv3(preds))))\n",
    "        preds = self.activation(self.max_pool2d(self.conv4batchnorm(self.conv4(preds))))\n",
    "        preds = self.activation(self.max_pool2d(self.conv5batchnorm(self.conv5(preds))))\n",
    "        preds = preds.view(-1,96*1*1)\n",
    "        preds = self.activation(self.linear1batchnorm(self.linear1(preds)))\n",
    "        preds = self.activation(self.linear2batchnorm(self.linear2(preds)))\n",
    "        preds = self.activation(self.linear3batchnorm(self.linear3(preds)))\n",
    "        preds = self.activation(self.linear4batchnorm(self.linear4(preds)))\n",
    "        preds = self.activation(self.linear5batchnorm(self.linear5(preds)))\n",
    "        preds = self.output(preds)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38300860-1220-48e0-82c5-7faf185c8cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85db0369-67b8-4b08-9721-6cd25095bd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96ff480f-ab4e-447b-b822-5a88e3740266",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecaaa392-5655-479f-9344-37d46e3d5737",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3558de5e-3d30-4a01-99d2-d0c0d530f25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fde1fc8a-9879-4fa6-8b81-f7bd90f43beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ce4a1f5-a422-4f6f-9cea-f4ec917432b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f2eb2de-6f57-4b75-80fe-665517fdf6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mranuga-d\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.1 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.0<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">baseline</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/ranuga-d/Satellite-Image-Classification\" target=\"_blank\">https://wandb.ai/ranuga-d/Satellite-Image-Classification</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/ranuga-d/Satellite-Image-Classification/runs/1gjh95co\" target=\"_blank\">https://wandb.ai/ranuga-d/Satellite-Image-Classification/runs/1gjh95co</a><br/>\n",
       "                Run data is saved locally in <code>/home/indika/Programming/Projects/Python/Artifical-Intelligence/PyTorch/CNN/Satellite-Image-Classification/wandb/run-20210902_201940-1gjh95co</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 34/100 [11:11<21:44, 19.76s/it]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'ndim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-4c203aa5a0c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./preds/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34mf'Img/{file}'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'./preds/{file}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/wandb/sdk/data_types.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_or_path, mode, caption, grouping, classes, boxes, masks)\u001b[0m\n\u001b[1;32m   1813\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1814\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1815\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_from_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1817\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_initialization_meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrouping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaption\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/wandb/sdk/data_types.py\u001b[0m in \u001b[0;36m_initialize_from_data\u001b[0;34m(self, data, mode)\u001b[0m\n\u001b[1;32m   1919\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"numpy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# TF data eager tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1921\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1922\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# get rid of trivial dimensions as a convenience\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m             self._image = pil_image.fromarray(\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'ndim'"
     ]
    }
   ],
   "source": [
    "wandb.init(project=PROJECT_NAME,name='baseline')\n",
    "wandb.watch(model)\n",
    "for _ in tqdm(range(epochs)):\n",
    "    for i in range(0,len(X_train),idx):\n",
    "        X_batch = X_train[i:i+batch_size].float().to(device).view(-1,3,112,112)\n",
    "        y_batch = y_train[i:i+batch_size].to(device)\n",
    "        model.to(device)\n",
    "        preds = model(X_batch)\n",
    "        loss = criterion(preds,y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    wandb.log({'Loss':get_loss(model,X_train,y_train,criterion)})\n",
    "    wandb.log({'Val Loss':get_loss(model,X_test,y_test,criterion)})\n",
    "    wandb.log({'Acc':get_accuracy(model,X_train,y_train)})\n",
    "    wandb.log({'Val Acc':get_accuracy(model,X_test,y_test)})\n",
    "    predict(model)\n",
    "    for file in os.listdir('./preds/'):\n",
    "        wandb.log({f'Img/{file}':wandb.Image(cv2.imread(f'./preds/{file}'))})\n",
    "wandb.watch(model)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120d36a5-3245-4bd1-8b4d-5b3680836d31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python373jvsc74a57bd0210f9608a45c0278a93c9e0b10db32a427986ab48cfc0d20c139811eb78c4bbc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
